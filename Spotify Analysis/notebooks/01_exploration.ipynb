{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 01 - Data Loading & Exploration\n",
    "\n",
    "**Purpose:** Load the Spotify dataset, understand its structure, and prepare it for analysis.\n",
    "\n",
    "- Data acquisition\n",
    "- Structure and schema overview\n",
    "- Data quality checks\n",
    "- Save cleaned dataset for subsequent notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_path = project_root / \"data\"\n",
    "sys.path.append(str(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotify_data import download_spotify_dataset\n",
    "\n",
    "data_dir = download_spotify_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = Path(\"../data/raw/spotify_analysis_dataset.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structure-header",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "info",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names and types:\")\n",
    "print(\"-\" * 40)\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing",
   "metadata": {},
   "outputs": [],
   "source": "#check for missing values\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df) * 100).round(2)\n\nmissing_df = pd.DataFrame({'missing_count': missing, 'missing_pct': missing_pct})\nmissing_df = missing_df[missing_df['missing_count'] > 0]\n\nif len(missing_df) > 0:\n    print(\"Columns with missing values:\")\n    print(missing_df)\nelse:\n    print(\"No missing values found.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicates",
   "metadata": {},
   "outputs": [],
   "source": "#check for duplicates\nduplicates = df.duplicated(subset=['track_id']).sum()\nprint(f\"Duplicate track_ids: {duplicates:,}\")\n\nif duplicates > 0:\n    print(\"Removing duplicates...\")\n    df = df.drop_duplicates(subset=['track_id'])\n    print(f\"New shape: {df.shape[0]:,} rows\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-counts",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Overview\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total tracks: {df['track_id'].nunique():,}\")\n",
    "print(f\"Unique artists: {df['artist'].nunique():,}\")\n",
    "print(f\"Unique albums: {df['album'].nunique():,}\")\n",
    "print(f\"Date range: {df['release_date'].min()} to {df['release_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prep-header",
   "metadata": {},
   "source": [
    "## Prepare Data for Analysis\n",
    "\n",
    "Add derived columns that will be used in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-columns",
   "metadata": {},
   "outputs": [],
   "source": "#convert and extract date features\ndf['release_date'] = pd.to_datetime(df['release_date'])\ndf['release_year'] = df['release_date'].dt.year\n\n#convert duration to minutes\ndf['duration_min'] = df['duration_ms'] / 60000\n\n#create popularity tiers\ndf['popularity_tier'] = pd.cut(df['popularity'], bins=[0, 33, 66, 100], \n                                labels=['Low', 'Medium', 'High'])\n\nprint(\"Added columns: release_year, duration_min, popularity_tier\")\nprint(f\"\\nPopularity tier distribution:\")\nprint(df['popularity_tier'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path(\"../data/processed\")\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = processed_path / \"spotify_cleaned.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved processed data to: {output_file}\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-header",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [02_feature_analysis.ipynb](02_feature_analysis.ipynb) - Deep dive into audio features and popularity correlations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}